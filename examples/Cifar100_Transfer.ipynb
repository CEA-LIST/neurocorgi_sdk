{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import n2d2\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets\n",
    "from neurocorgi_sdk.models import NeuroCorgiNet_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If possible, set up the GPU 0 for the application\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "n2d2.global_variables.default_model = \"Frame_CUDA\" if n2d2.global_variables.cuda_available else \"Frame\"\n",
    "n2d2.global_variables.cuda_device = 0\n",
    "\n",
    "n2d2.global_variables.verbosity = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_transforms = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       ])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      ])\n",
    "\n",
    "dataset_train = torchvision.datasets.CIFAR100(root=\"./data\", train=True, download=True, transform=train_transforms)\n",
    "dataset_test = torchvision.datasets.CIFAR100(root=\"./data\", train=False, download=True, transform=test_transforms)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=128, shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=128, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im_convert(tensor):\n",
    "  image = tensor.cpu().clone().detach().numpy()\n",
    "  image = image.transpose(1, 2, 0)\n",
    "  image = image.clip(0, 1)\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = train_loader.dataset.classes\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "\n",
    "for idx in np.arange(20):\n",
    "  ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n",
    "  plt.imshow(im_convert(images[idx]))\n",
    "  ax.set_title(classes[labels[idx].item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this example, we use the model pretrained and quantized with the ImageNet dataset\n",
    "extractor = NeuroCorgiNet_torch([128, 3, 224, 224], weights_dir=\"data/imagenet_weights\")\n",
    "extractor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = torch.nn.AvgPool2d(7, stride=7)\n",
    "flatten = torch.nn.Flatten(start_dim=1)\n",
    "classifier = torch.nn.Linear(1024, 100)\n",
    "head = torch.nn.Sequential(pool, flatten, classifier)\n",
    "head.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(head.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = extractor.eval()\n",
    "head = head.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "val_running_loss_history = []\n",
    "val_running_corrects_history = []\n",
    "\n",
    "for e in range(epochs):\n",
    "  \n",
    "  running_loss = 0.0\n",
    "  running_corrects = 0.0\n",
    "  val_running_loss = 0.0\n",
    "  val_running_corrects = 0.0\n",
    "  \n",
    "  for i, (inputs, labels) in enumerate(tqdm(train_loader)):\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    conv3_1x1, conv5_1x1, conv7_5_1x1, conv9_1x1 = extractor(inputs)\n",
    "    outputs = head(conv9_1x1)\n",
    "\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    running_loss += loss.item()\n",
    "    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "  else:\n",
    "      with torch.no_grad():\n",
    "        for i, (val_inputs, val_labels) in enumerate(tqdm(test_loader)):\n",
    "          val_inputs = val_inputs.to(device)\n",
    "          val_labels = val_labels.to(device)\n",
    "\n",
    "          conv3_1x1, conv5_1x1, conv7_5_1x1, conv9_1x1 = extractor(val_inputs)\n",
    "          val_outputs = head(conv9_1x1)\n",
    "\n",
    "          val_loss = criterion(val_outputs, val_labels)\n",
    "          \n",
    "          _, val_preds = torch.max(val_outputs, 1)\n",
    "          val_running_loss += val_loss.item()\n",
    "          val_running_corrects += torch.sum(val_preds == val_labels.data)\n",
    "        \n",
    "      epoch_loss = running_loss/len(train_loader.dataset)\n",
    "      epoch_acc = running_corrects.float()/ len(train_loader.dataset)\n",
    "      running_loss_history.append(epoch_loss)\n",
    "      running_corrects_history.append(epoch_acc)\n",
    "      \n",
    "      val_epoch_loss = val_running_loss/len(test_loader.dataset)\n",
    "      val_epoch_acc = val_running_corrects.float()/ len(test_loader.dataset)\n",
    "      val_running_loss_history.append(val_epoch_loss)\n",
    "      val_running_corrects_history.append(val_epoch_acc)\n",
    "      print('epoch :', (e+1))\n",
    "      print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "      print('validation loss: {:.4f}, validation acc {:.4f} '.format(val_epoch_loss, val_epoch_acc.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(running_loss_history, label='training loss')\n",
    "plt.plot(val_running_loss_history, label='validation loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_corrects_history = [x.to(\"cpu\") for x in running_corrects_history]\n",
    "val_running_corrects_history = [x.to(\"cpu\") for x in val_running_corrects_history]\n",
    "\n",
    "plt.plot(running_corrects_history, label='training accuracy')\n",
    "plt.plot(val_running_corrects_history, label='validation accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = extractor.eval()\n",
    "head = head.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_corrects = 0.0\n",
    "  \n",
    "for inputs, labels in test_loader:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    conv3_1x1, conv5_1x1, conv7_5_1x1, conv9_1x1 = extractor(inputs)\n",
    "    outputs = head(conv9_1x1)\n",
    "\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "epoch_acc = running_corrects.float()/ len(train_loader)\n",
    "print(epoch_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "images = images.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "conv3_1x1, conv5_1x1, conv7_5_1x1, conv9_1x1 = extractor(images)\n",
    "output = head(conv9_1x1)\n",
    "_, preds = torch.max(output, 1)\n",
    "\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "\n",
    "for idx in np.arange(20):\n",
    "  ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n",
    "  plt.imshow(im_convert(images[idx]))\n",
    "  ax.set_title(\"{} ({})\".format(str(classes[preds[idx].item()]), str(classes[labels[idx].item()])), color=(\"green\" if preds[idx]==labels[idx] else \"red\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a948bba95821b0c9c13943546c3ceee9666ce731755beb42894160f547c6eef1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
